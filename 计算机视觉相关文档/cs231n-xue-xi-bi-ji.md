# CS231n学习笔记

#### 01.视觉识别CNN简介

* 从 20 世纪 60 年代末到 2017 年的计算机视觉简史。
* 计算机视觉问题包括图像分类、对象定位、对象检测和场景理解。
* [Imagenet](http://www.image-net.org/)是目前最大的图像分类数据集之一。
* 从 2012 年开始，在 Imagenet 竞赛中，CNN（卷积神经网络）始终获胜。
* CNN 实际上是由[Yann Lecun](http://ieeexplore.ieee.org/document/726791/)于 1997 年发明的。

#### 02.图像分类

图像分类问题具有许多挑战，如光照和视角变化。K最近邻（KNN）算法可以用来解决图像分类问题，但效果可能不好。KNN的一些属性包括：超参数k和距离度量。其中，k代表要比较的最近邻数量，而距离度量可以使用[L2距离（欧氏距离）或L1距离（曼哈顿距离）](https://blog.csdn.net/weixin\_43135178/article/details/115426550)。

为了优化超参数，可以使用交叉验证方法：将数据集分成f个折叠（f-folds），利用f-1个折叠进行训练并在剩余的折叠上测试，然后对所有折叠的结果进行平均，选择表现最佳的超参数。

线性支持向量机（SVM）分类器是解决图像分类问题的另一个选择，但维度的灾难使其在某个点停止改进。

逻辑回归也可用于图像分类问题，但图像分类问题是非线性的。

线性分类器需要运行以下公式：Y = wX + b，其中w和b是待求的参数。

为了消除偏差项，我们可以向X向量添加一个元素，并将偏差表示为常数项，这样就可以得到Y = wX的形式。

我们需要找到最佳的w和b值，使分类器能够最好地运行。

#### 03. 损失函数和优化

在上一节中，我们讨论了线性分类器，但我们没有讨论如何训练模型的参数以获得最佳的权重和偏置。

为了衡量当前参数的好坏，我们需要一个损失函数。

对于线性SVM分类器的损失函数，我们称之为"_**the hinge loss**_."： L\[i] = Sum（除了预测类别以外的所有类别）（max（0，s\[j] - s\[y\[i]] + 1））

损失函数意味着如果最佳预测与真实值相同，我们就满意，否则我们给出1个单位的错误。我们希望最小化这个损失。

损失函数为零并不意味着参数是唯一的最佳解，可能会有多个参数组合可以得到相同的最佳分数。

我们还通过正则化来防止过拟合的发生。正则化将于损失函数相加，避免了模型过度拟合数据。常见的正则化方法有L1正则化、L2正则化、Elastic net (L1 + L2)正则化和Dropout。

**简单来说，L1/L2正则化就是在机器学习/深度学习中应用了L1/L2范数，具体来说，就是在损失函数loss上增加了L1或L2范数项，达到参数惩罚的作用，即实现了正则化的效果，从而称为L1/L2正则化。**\


正则化倾向于较小的参数权重，而不是较大的权重。正则化有时也被称为权重衰减，偏差不应包括在正则化中。

Softmax损失用于多分类问题，类似于线性回归，但适用于多于2个类别。Softmax函数用于将分数转换为概率，通过计算-logP(Y=y\[i]|X=x\[i])来计算损失。\


具体过程如下：

* 在分类任务中，我们使用条件概率 P(Y=y\[i]|X=x\[i]) 来表示给定输入特征 X=x\[i] 的情况下，输出类别为 y\[i] 的概率。
* 为了训练模型并提高预测准确性，我们使用负对数似然损失函数 -logP(Y=y\[i]|X=x\[i]) 来度量模型的预测与真实类别之间的差异。
* 负对数似然损失函数惩罚模型对于错误类别的预测，通过累加所有样本的损失值来得到总的损失函数值。
* 在训练过程中，我们使用优化算法最小化损失函数，调整模型的参数以提高预测准确性。
* 通过不断迭代优化和更新参数，模型将逐渐学会更准确地估计条件概率 P(Y=y\[i]|X=x\[i])，从而提升分类任务的性能。



在优化过程中，我们可以使用梯度下降法来最小化损失函数。梯度下降法的目标是沿着损失函数的斜率方向更新参数。通过计算参数的梯度，我们可以进行梯度下降的更新。学习率是一个重要的超参数，需要根据实际情况选择合适的值。

除了传统的梯度下降法，还有随机梯度下降法（SGD）。SGD每次只使用一小批样本来更新参数，从而加快了训练速度。

总结起来，我们需要选择合适的损失函数来衡量模型的性能，并通过优化算法如梯度下降法来最小化损失函数，以获得最佳的参数权重和偏置。正则化可以用来防止过拟合问题。

#### 04.神经网络简介

神经网络是由一系列简单操作组成的堆叠，形成复杂的操作。计算图被用于表示任何函数，它通过节点将函数的计算过程划分为更小的操作单元。在计算图中，我们可以使用反向传播算法来计算每个参数的梯度，进而优化模型。

在计算图中，每个操作被称为`f`，我们首先计算每个操作的局部梯度，然后使用链式法则来计算相对于损失函数的梯度。通过将操作分解为更简单的操作，我们可以获得更多的节点，但也需要能够计算每个节点的梯度。

在思考神经网络时，我们可以将其视为一个函数。在线性评分函数之后，现在我们使用2层或更多层的神经网络来构建更复杂的操作。例如，在2层神经网络中，我们使用ReLU非线性函数和权重矩阵来定义函数。通过堆叠更多的层，我们可以构建出更深层次的神经网络。

神经网络是深度学习框架的基础，每个类都有前向传播和反向传播的定义。这种模块化实现使得神经网络的搭建和训练更加简洁明了。常见的操作包括乘法、最大值、加法、减法、sigmoid和卷积等。

#### 05.卷积神经网络（CNN）

卷积神经网络（CNN）是一种用于处理图像数据的神经网络模型。CNN的发展历史可以追溯到20世纪50年代，但直到近年来才取得了重大突破。

CNN最早的概念是由Hubel和Wisel在20世纪50年代至60年代的实验中提出的，他们发现大脑皮层中存在着对图像的层次化处理机制。然而，在1998年，Yann Lecun发表的一篇论文引入了卷积神经网络的概念，并针对手写字符识别取得了一定的成功。

直到2012年，AlexNet的问世才真正将CNN引入了主流应用领域。AlexNet在ImageNet图像识别竞赛中取得了重大突破，这也标志着CNN在计算机视觉任务中的应用开始迅速扩展。

CNN的核心思想是通过卷积层、激活函数层、池化层等不同类型的层次构建神经网络，以处理图像数据。卷积层通过滤波器与输入层进行卷积运算，提取特征并保留输入数据的结构。激活函数层通过引入非线性变换，增加网络的表达能力。池化层则通过减小特征图的尺寸，降低计算负载并增强网络对平移不变性的感知能力。

在卷积操作中，步幅（stride）的选择决定了滤波器在输入数据上的移动步长。使用合适的步幅和填充方式，可以保持输出数据的尺寸与输入数据相同。同时，填充操作还可以增加输入数据的边缘特征。\


1.  在进行卷积操作时，我们可以根据需要选择步长（stride）。步长指的是在滑动过程中的跳跃次数，默认值为1。以一个形状为(7,7)的矩阵和一个形状为(3,3)的滤波器为例进行解释：

    * 如果步长为1，则输出的形状是(5,5)，即有2个元素被丢弃。
    * 如果步长为2，则输出的形状是(3,3)，即有4个元素被丢弃。
    * 如果步长为3，则无法完成滑动计算。

    一般的计算公式为((N-F)/stride + 1)，其中N是输入的维度，F是滤波器的维度，O是输出的维度。

    * 如果步长为1，则 O = ((7-3)/1) + 1 = 4 + 1 = 5。
    * 如果步长为2，则 O = ((7-3)/2) + 1 = 2 + 1 = 3。
    * 如果步长为3，则 O = ((7-3)/3) + 1 = 1.33 + 1 = 2.33，这种情况下无法完成计算。

    综上所述，步长的选择会影响输出的形状和丢失的元素数量。
2. 在卷积操作中，常常会对边界进行零填充（padding），以保持输入的完整大小。
   * 当步长为1时，通常会使用(F-1)/2的公式对边界进行填充，其中F是滤波器的大小。例如，当滤波器大小为3时，会进行1个单位的零填充；当滤波器大小为5时，会进行2个单位的零填充。
   * 这种填充方式可以防止输入数据过快地缩小，从而丢失大量信息。零填充还可以为边缘区域提供额外的特征。虽然存在其他填充技术，如角落填充，但在实践中，零填充是最常用的选择。



除了卷积层外，池化层也是CNN中常用的一种操作。最大池化是一种常见的池化方法，它通过选取特征图中的最大值来减小尺寸。平均池化是另一种池化方法，它通过计算特征图中数值的平均值来实现尺寸的缩减。

总而言之，CNN以其对图像数据的处理能力，在图像分类、目标检测、图像分割、人脸识别、医学图像等多个领域取得了突出的成果，并成为深度学习中重要的模型之一。

#### 06.训练神经网络（一）

迷你批量随机梯度下降算法的步骤:

* 采样一批数据
* 将数据通过网络进行前向传播并计算损失
* 反向传播以计算梯度
* 使用梯度更新参数

常见的激活函数包括[sigmoid、tanh、ReLU、leaky ReLU、ELU和Maxout](https://zhuanlan.zhihu.com/p/260970955)：

* Sigmoid将数值压缩到0和1之间，但存在梯度消失问题，且不是以零为中心
* Tanh将数值压缩到-1和1之间，它以零为中心，但仍然存在梯度消失问题
* ReLU（线性整流单元）在计算上更高效，收敛速度比sigmoid和tanh更快，并且更符合生物实际情况，但它不是以零为中心
* Leaky ReLU和ELU是ReLU的变种，解决了其非零中心的问题
* Maxout是ReLU和Leaky ReLU的推广，但会增加每个神经元的参数数量

数据预处理包括归一化操作，使数据变为零均值和单位方差。

对神经网络进行权重初始化很重要：

* 将权重初始化为零会导致所有神经元的行为相同
* Xavier初始化和He初始化是常用的权重初始化技术，其中推荐在使用ReLU激活函数的网络中使用He初始化

批归一化是一种对神经网络每一层输入进行归一化的技术，它可以加速训练、允许更高的学习率，并提供正则化效果。

"宝宝式"监督学习过程包括数据预处理、选择架构、检查无正则化的损失、对小数据集进行过度拟合、找到适当的学习率和优化超参数。

超参数优化可以通过交叉验证和在对数空间中进行的随机搜索来实现。

#### 07.训练神经网络（二）

优化算法方面，讨论了随机梯度下降（SGD）存在的问题，如在某些维度上进展缓慢而在其他维度上进展快速时会导致进展缓慢；局部最小值或鞍点问题，因为梯度为零会导致停滞；以及高维空间中更多涉及鞍点而非局部最小值的问题。针对这些问题，介绍了SGD+动量、Nestrov动量、AdaGrad、RMSProp和Adam等优化算法的原理和应用场景。

* SGD + 动量：通过积累梯度的动量来解决随机梯度下降中的问题。使用一个权重衰减因子来建立梯度的运行平均值，从而避免梯度方向变化剧烈的问题。
* Nestrov 动量：对 SGD + 动量进行改进，避免了过度修正问题，但相对而言速度较慢。
* AdaGrad：针对学习率难以设置的问题，通过缩放梯度的步长来自适应地调整学习率，但会出现累积梯度平方项过大的问题。
* RMSProp：在 AdaGrad 的基础上进行改进，通过引入衰减因子来解决累积梯度平方项过大的问题，更常用于实际应用。
* Adam：结合了动量和 RMSProp 的优点，可以视为动量和 RMSProp 的组合，并具有一些优化的细节处理，被认为是目前效果最好的优化算法之一。

正则化技术方面，探讨了模型集成、L1和L2正则化等常见技术，并介绍了一些专门设计用于神经网络的正则化技术，如Dropout、数据增强、DropConnect、Fractional Max Pooling和Stochastic Depth等。

* 模型集成：训练多个独立模型，并在测试时对它们的结果进行平均，以减少过拟合的问题。
* Dropout：在每次前向传播时，随机将一部分神经元置零，强制网络具备冗余表示，避免特征的共适应。
* 数据增强：通过对数据进行各种变换（如翻转、旋转等）来扩充数据集，增加了样本多样性，有助于提高模型的泛化能力。
* DropConnect：类似于 Dropout 的思想，但是将随机置零的对象从神经元转变为权重参数。
* Fractional Max Pooling：一种罕见但有效的正则化方法，随机选择池化区域，增加了模型的鲁棒性。
* Stochastic Depth：相对于神经元级别的 Dropout，它是对层级进行随机丢弃，具有类似 Dropout 的效果，但是是一种新的思路。

此外，还介绍了迁移学习的方法，包括在大型数据集上进行预训练，冻结除最后一层以外的层并在小型数据集上微调模型等步骤。对于不同类型的数据集和数据量，提供了相应的迁移学习策略指导。

* 预训练：首先在一个具有相似特征的大型数据集上进行训练，然后将这个已训练好的模型应用于目标数据集。
* 微调：冻结除最后一层以外的层，并在小型目标数据集上重新训练最后一层或几层，以适应目标任务。

#### 08.深度学习软件

* CPU与GPU：GPU适合并行任务，NVIDIA是深度学习首选。
* 深度学习框架：Tensorflow、PyTorch等是常用框架。
* Tensorflow（谷歌）：使用静态图，有全局初始化函数和预定义优化器、损失函数等。
* PyTorch（Facebook）：使用动态图，具有自动求导功能，适合研究。
* 高级封装库：Keras等简化模型构建和训练过程。
* 预训练模型：Tensorflow和PyTorch提供预训练模型。
* GPU编程框架：CUDA是常用的GPU编程框架。
* 分布式计算：Tensorflow支持分布式计算。
* 其他框架：Caffe、Caffe2等也常用于深度学习。
* 深度学习软件在生产环境和移动设备上广泛应用。

#### 09.CNN架构

1. AlexNet：
   * 层数：8层（5个卷积层 + 3个全连接层）
   * 特点：引入了ReLU激活函数和Dropout正则化技术
   * 成就：2012年获得ImageNet竞赛冠军，标志着深度学习在计算机视觉领域的突破
2. VGGNet：
   * 层数：19层
   * 特点：使用了小尺寸的3x3卷积核和堆叠的方式增加网络的深度
   * 成就：提出了深度即特征表达的重要性，在深度学习发展中具有里程碑意义
3. GoogLeNet：
   * 层数：22层
   * 特点：引入了Inception模块，有效处理了多尺度和滑动窗口方法
   * 成就：2014年获得ImageNet竞赛冠军，开创了多分支结构和网络拓扑的先河
4. ResNet：
   * 层数：152层（具体的版本可以有所不同）
   * 特点：引入了残差连接，解决了深度网络训练中的梯度消失问题
   * 成就：2015年获得ImageNet竞赛冠军，对于非常深的神经网络具有重要意义

#### 10.循环神经网络

循环神经网络（RNN）是一种能够处理序列数据的神经网络模型。它通过在每个时间步骤上应用一个递归函数来处理输入序列，并且具有一个内部状态，该状态会根据输入进行更新。RNN可以用于处理各种类型的序列数据，包括文本、语音和时间序列数据等。

RNN模型有多种变体，包括一对多、多对一和多对多等不同结构。例如，可以将图像转换为描述图像内容的文字序列（一对多），或者将一个句子分类为情感类别（多对一），还可以进行机器翻译（多对多）等任务。

RNN不仅能够处理序列数据，还可以用于非序列数据的任务。例如，可以使用RNN模型对手写数字进行分类，或者生成一张图片的验证码等。

一个基本的RNN单元包含一个递归的核心单元，该单元接收输入并更新内部状态。通过在每个时间步骤上应用相同的函数和参数，可以处理一个序列的向量。常见的RNN单元是使用tanh作为激活函数，其中输出向量用于下一个时间步骤的计算。RNN的训练过程通常使用反向传播算法，通过沿着时间步骤的前向计算和反向传播来计算梯度，并更新参数以最小化损失函数。

在实际应用中，通常采用“截断反向传播算法”来应对RNN训练过程中的梯度计算问题。这种方法将序列分割为多个片段，在每个片段上进行前向计算和反向传播，而不是在整个序列上进行。此外，为了应对梯度爆炸的问题，可以使用梯度裁剪技术；为了应对梯度消失的问题，可以使用具有长期记忆能力的LSTM（长短期记忆）单元。

总而言之，RNN是一种适用于处理序列数据的神经网络模型，广泛应用于自然语言处理、语音识别等领域。随着研究的深入，人们对于RNN结构和性能的理解也在不断加深，同时也在探索更好的网络结构和训练算法。
