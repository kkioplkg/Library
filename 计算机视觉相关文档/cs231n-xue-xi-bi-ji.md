# CS231n学习笔记

#### 01.视觉识别CNN简介

* 从 20 世纪 60 年代末到 2017 年的计算机视觉简史。
* 计算机视觉问题包括图像分类、对象定位、对象检测和场景理解。
* [Imagenet](http://www.image-net.org/)是目前最大的图像分类数据集之一。
* 从 2012 年开始，在 Imagenet 竞赛中，CNN（卷积神经网络）始终获胜。
* CNN 实际上是由[Yann Lecun](http://ieeexplore.ieee.org/document/726791/)于 1997 年发明的。

#### 02.图像分类

* 图像分类问题有很多挑战，例如光照和视点。
  *
* 图像分类算法可以用K近邻（KNN）来解决，但解决问题的效果很差。KNN 的性质是：
  * KNN 的超参数为：k 和距离度量
  * K 是我们要比较的邻居的数量。
  * 距离测量包括：
    * L2距离（欧氏距离）
      * 最适合非坐标点
    * L1距离（曼哈顿距离）
      * 最适合坐标点
* 可以使用交叉验证来优化超参数，如下所示（在我们的例子中，我们正在尝试预测 K）：
  1. 将数据集拆分为`f`折叠。
  2. 给定预测的超参数：
     * 使用 f-1 折叠训练您的算法并使用剩余的一个折叠对其进行测试。并在每次折叠时重复此操作。
  3. 选择提供最佳训练值的超参数（所有折叠的平均值）
* **线性 SVM**分类器是解决图像分类问题的一种选择，但在高维情况下可能效果不佳。
* **逻辑回归**也是图像分类问题的一种解决方案，但图像分类问题通常是非线性的！
*   线性分类器必须运行以下方程：

    ```
     Y = wX + b
    ```

    * 其中w的形状与x相同，而b的形状为1。
*   我们可以将 1 添加到 X 向量并消除偏差，以便：

    ```
     Y = wX
    ```

    * 其中x的形状为(oldX+1)，w的形状与x相同
* 我们需要知道如何获得`w`和`b`以使分类器运行得最好。

#### 03. 损失函数和优化

*   在上一节中，我们讨论了线性分类器，但并没有讨论如何训练模型的参数以获得最佳的`w`和`b`。为了衡量当前参数的好坏，我们需要一个损失函数。

    损失函数用于衡量预测结果与真实值之间的差距，常用的形式如下：

    ```
     Loss = L[i] = (f(X[i],W),Y[i])
     Loss_for_all = 1/N * Sum(Li(f(X[i],W),Y[i]))  # 表示平均损失
    ```

    然后，我们需要找到将损失函数最小化的方法，这称为优化。

    线性SVM分类器的损失函数如下：

    `L[i] = Sum(除预测类别外的所有类别( max(0, s[j] - s[y[i]] + 1 ) ) )`

    我们称这个函数为**合页损失**。损失函数意味着如果最佳预测与真实值相同，我们就满意，否则我们会给出一个错误，并给予1的边界。例如，给定一个示例图片，我们希望计算出该图片的损失。根据公式计算即可得到最终的损失。

    如果损失函数的值为零，这个值是否对应了最佳参数？并不是，还有很多参数组合可以得到相同的最佳分数。

    有时人们会使用平方合页损失SVM（或L2-SVM），它对违反边界的惩罚更加严厉（二次而不是线性）。未平方的版本更为常见，但在某些数据集上，平方合页损失可能效果更好。

    我们通过为损失函数添加**正则化**来防止模型对数据过拟合。正则化后的损失函数如下：

    ```
     Loss = L = 1/N * Sum(Li(f(X[i],W),Y[i])) + lambda * R(W)
    ```

    其中，`R`是正则项，`lambda`是正则化系数。

    常用的正则化技术有：

    | 正则项     | 方程式                                   | 备注         |
    | ------- | ------------------------------------- | ---------- |
    | L2正则化   | `R(W) = Sum(W^2)`                     | 所有`W`的平方和  |
    | L1正则化   | `R(W) = Sum(\|W\|)`                   | 所有`W`的绝对值和 |
    | 弹性网正则化  | `R(W) = beta * Sum(W^2) + Sum(\|W\|)` |            |
    | Dropout |                                       | 没有具体方程式    |

    正则化更倾向于较小的权重`W`而非较大的权重。

    正则化也被称为权重衰减，偏置项不应包含在正则化中。

    对于多于两个类别的情况，softmax损失函数可用于分类：

    *   softmax函数：

        ```
         A[L] = e^(score[L]) / sum(e^(score[L]), NoOfClasses)
        ```

        向量之和应为1。
    *   softmax损失函数：

        ```
         Loss = -logP(Y = y[i]|X = x[i])
        ```

        是好类别的概率的负对数。希望它接近1，因此加了一个负号。这个损失函数也被称为交叉熵损失。

    在计算softmax时要考虑数值问题，可以通过以下方式解决：

    ```
     f = np.array([123, 456, 789]) # 有3个类别，每个类别具有较大的得分
     p = np.exp(f) / np.sum(np.exp(f)) # 错误：数值问题，可能会发生溢出
     ​
     # 改进：先将f中的值都减去最大值，使得最大值为0：
     f -= np.max(f) # f变为[-666, -333, 0]
     p = np.exp(f) / np.sum(np.exp(f)) # 正确计算结果，避免了数值问题
    ```

    **优化**：

    我们如何优化前面讨论的损失函数？以下是两种策略：

    * 策略一：随机初始化参数，尝试所有参数组合，并选择使得损失最小的组合。但这样做效果不好。
    *   策略二：沿着斜坡方向前进。

        我们的目标是计算每个参数的梯度：

        * **数值梯度**：近似值，计算慢，编写简单（在调试中很有用）。
        * **解析梯度**：精确值，计算快，易出错（实际应用中常用）。

        计算完参数的梯度后，我们使用梯度下降法进行优化：

        ```
         W = W - learning_rate * W_grad
        ```

        学习率是非常重要的超参数，要先确定最佳值。

        随机梯度下降（stochastic gradient descent）：

        不使用所有数据，而是使用一个小批量的样本（通常是32/64/128个样本）来加快计算速度。

#### 04.神经网络简介

神经网络是由一系列简单操作组成的堆叠，形成复杂的操作。计算图被用于表示任何函数，它通过节点将函数的计算过程划分为更小的操作单元。在计算图中，我们可以使用反向传播算法来计算每个参数的梯度，进而优化模型。

在计算图中，每个操作被称为`f`，我们首先计算每个操作的局部梯度，然后使用链式法则来计算相对于损失函数的梯度。通过将操作分解为更简单的操作，我们可以获得更多的节点，但也需要能够计算每个节点的梯度。

在思考神经网络时，我们可以将其视为一个函数。在线性评分函数之后，现在我们使用2层或更多层的神经网络来构建更复杂的操作。例如，在2层神经网络中，我们使用ReLU非线性函数和权重矩阵来定义函数。通过堆叠更多的层，我们可以构建出更深层次的神经网络。

神经网络是深度学习框架的基础，每个类都有前向传播和反向传播的定义。这种模块化实现使得神经网络的搭建和训练更加简洁明了。常见的操作包括乘法、最大值、加法、减法、sigmoid和卷积等。

#### 05.卷积神经网络（CNN）

卷积神经网络（CNN）是一种用于处理图像数据的神经网络模型。CNN的发展历史可以追溯到20世纪50年代，但直到近年来才取得了重大突破。

CNN最早的概念是由Hubel和Wisel在20世纪50年代至60年代的实验中提出的，他们发现大脑皮层中存在着对图像的层次化处理机制。然而，在1998年，Yann Lecun发表的一篇论文引入了卷积神经网络的概念，并针对手写字符识别取得了一定的成功。

直到2012年，AlexNet的问世才真正将CNN引入了主流应用领域。AlexNet在ImageNet图像识别竞赛中取得了重大突破，这也标志着CNN在计算机视觉任务中的应用开始迅速扩展。

CNN的核心思想是通过卷积层、激活函数层、池化层等不同类型的层次构建神经网络，以处理图像数据。卷积层通过滤波器与输入层进行卷积运算，提取特征并保留输入数据的结构。激活函数层通过引入非线性变换，增加网络的表达能力。池化层则通过减小特征图的尺寸，降低计算负载并增强网络对平移不变性的感知能力。

在卷积操作中，步幅（stride）的选择决定了滤波器在输入数据上的移动步长。使用合适的步幅和填充方式，可以保持输出数据的尺寸与输入数据相同。同时，填充操作还可以增加输入数据的边缘特征。

除了卷积层外，池化层也是CNN中常用的一种操作。最大池化是一种常见的池化方法，它通过选取特征图中的最大值来减小尺寸。平均池化是另一种池化方法，它通过计算特征图中数值的平均值来实现尺寸的缩减。

总而言之，CNN以其对图像数据的处理能力，在图像分类、目标检测、图像分割、人脸识别、医学图像等多个领域取得了突出的成果，并成为深度学习中重要的模型之一。

#### 06.训练神经网络（一）

迷你批量随机梯度下降算法的步骤:

* 采样一批数据
* 将数据通过网络进行前向传播并计算损失
* 反向传播以计算梯度
* 使用梯度更新参数

常见的激活函数包括sigmoid、tanh、ReLU、leaky ReLU、ELU和Maxout：

* Sigmoid将数值压缩到0和1之间，但存在梯度消失问题，且不是以零为中心
* Tanh将数值压缩到-1和1之间，它以零为中心，但仍然存在梯度消失问题
* ReLU（线性整流单元）在计算上更高效，收敛速度比sigmoid和tanh更快，并且更符合生物实际情况，但它不是以零为中心
* Leaky ReLU和ELU是ReLU的变种，解决了其非零中心的问题
* Maxout是ReLU和Leaky ReLU的推广，但会增加每个神经元的参数数量

数据预处理包括归一化操作，使数据变为零均值和单位方差。

对神经网络进行权重初始化很重要：

* 将权重初始化为零会导致所有神经元的行为相同
* Xavier初始化和He初始化是常用的权重初始化技术，其中推荐在使用ReLU激活函数的网络中使用He初始化

批归一化是一种对神经网络每一层输入进行归一化的技术，它可以加速训练、允许更高的学习率，并提供正则化效果。

"宝宝式"监督学习过程包括数据预处理、选择架构、检查无正则化的损失、对小数据集进行过度拟合、找到适当的学习率和优化超参数。

超参数优化可以通过交叉验证和在对数空间中进行的随机搜索来实现。

#### 07.训练神经网络（二）

优化算法方面，讨论了随机梯度下降（SGD）存在的问题，如在某些维度上进展缓慢而在其他维度上进展快速时会导致进展缓慢；局部最小值或鞍点问题，因为梯度为零会导致停滞；以及高维空间中更多涉及鞍点而非局部最小值的问题。针对这些问题，介绍了SGD+动量、Nestrov动量、AdaGrad、RMSProp和Adam等优化算法的原理和应用场景。

* SGD + 动量：通过积累梯度的动量来解决随机梯度下降中的问题。使用一个权重衰减因子来建立梯度的运行平均值，从而避免梯度方向变化剧烈的问题。
* Nestrov 动量：对 SGD + 动量进行改进，避免了过度修正问题，但相对而言速度较慢。
* AdaGrad：针对学习率难以设置的问题，通过缩放梯度的步长来自适应地调整学习率，但会出现累积梯度平方项过大的问题。
* RMSProp：在 AdaGrad 的基础上进行改进，通过引入衰减因子来解决累积梯度平方项过大的问题，更常用于实际应用。
* Adam：结合了动量和 RMSProp 的优点，可以视为动量和 RMSProp 的组合，并具有一些优化的细节处理，被认为是目前效果最好的优化算法之一。

正则化技术方面，探讨了模型集成、L1和L2正则化等常见技术，并介绍了一些专门设计用于神经网络的正则化技术，如Dropout、数据增强、DropConnect、Fractional Max Pooling和Stochastic Depth等。

* 模型集成：训练多个独立模型，并在测试时对它们的结果进行平均，以减少过拟合的问题。
* Dropout：在每次前向传播时，随机将一部分神经元置零，强制网络具备冗余表示，避免特征的共适应。
* 数据增强：通过对数据进行各种变换（如翻转、旋转等）来扩充数据集，增加了样本多样性，有助于提高模型的泛化能力。
* DropConnect：类似于 Dropout 的思想，但是将随机置零的对象从神经元转变为权重参数。
* Fractional Max Pooling：一种罕见但有效的正则化方法，随机选择池化区域，增加了模型的鲁棒性。
* Stochastic Depth：相对于神经元级别的 Dropout，它是对层级进行随机丢弃，具有类似 Dropout 的效果，但是是一种新的思路。

此外，还介绍了迁移学习的方法，包括在大型数据集上进行预训练，冻结除最后一层以外的层并在小型数据集上微调模型等步骤。对于不同类型的数据集和数据量，提供了相应的迁移学习策略指导。

* 预训练：首先在一个具有相似特征的大型数据集上进行训练，然后将这个已训练好的模型应用于目标数据集。
* 微调：冻结除最后一层以外的层，并在小型目标数据集上重新训练最后一层或几层，以适应目标任务。

#### 08.深度学习软件

* CPU与GPU：GPU适合并行任务，NVIDIA是深度学习首选。
* 深度学习框架：Tensorflow、PyTorch等是常用框架。
* Tensorflow（谷歌）：使用静态图，有全局初始化函数和预定义优化器、损失函数等。
* PyTorch（Facebook）：使用动态图，具有自动求导功能，适合研究。
* 高级封装库：Keras等简化模型构建和训练过程。
* 预训练模型：Tensorflow和PyTorch提供预训练模型。
* GPU编程框架：CUDA是常用的GPU编程框架。
* 分布式计算：Tensorflow支持分布式计算。
* 其他框架：Caffe、Caffe2等也常用于深度学习。
* 深度学习软件在生产环境和移动设备上广泛应用。

#### 09.CNN架构

最

1. AlexNet：
   * 层数：8层（5个卷积层 + 3个全连接层）
   * 特点：引入了ReLU激活函数和Dropout正则化技术
   * 成就：2012年获得ImageNet竞赛冠军，标志着深度学习在计算机视觉领域的突破
2. VGGNet：
   * 层数：19层
   * 特点：使用了小尺寸的3x3卷积核和堆叠的方式增加网络的深度
   * 成就：提出了深度即特征表达的重要性，在深度学习发展中具有里程碑意义
3. GoogLeNet：
   * 层数：22层
   * 特点：引入了Inception模块，有效处理了多尺度和滑动窗口方法
   * 成就：2014年获得ImageNet竞赛冠军，开创了多分支结构和网络拓扑的先河
4. ResNet：
   * 层数：152层（具体的版本可以有所不同）
   * 特点：引入了残差连接，解决了深度网络训练中的梯度消失问题
   * 成就：2015年获得ImageNet竞赛冠军，对于非常深的神经网络具有重要意义

#### 10.循环神经网络

循环神经网络（RNN）是一种能够处理序列数据的神经网络模型。它通过在每个时间步骤上应用一个递归函数来处理输入序列，并且具有一个内部状态，该状态会根据输入进行更新。RNN可以用于处理各种类型的序列数据，包括文本、语音和时间序列数据等。

RNN模型有多种变体，包括一对多、多对一和多对多等不同结构。例如，可以将图像转换为描述图像内容的文字序列（一对多），或者将一个句子分类为情感类别（多对一），还可以进行机器翻译（多对多）等任务。

RNN不仅能够处理序列数据，还可以用于非序列数据的任务。例如，可以使用RNN模型对手写数字进行分类，或者生成一张图片的验证码等。

一个基本的RNN单元包含一个递归的核心单元，该单元接收输入并更新内部状态。通过在每个时间步骤上应用相同的函数和参数，可以处理一个序列的向量。常见的RNN单元是使用tanh作为激活函数，其中输出向量用于下一个时间步骤的计算。RNN的训练过程通常使用反向传播算法，通过沿着时间步骤的前向计算和反向传播来计算梯度，并更新参数以最小化损失函数。

在实际应用中，通常采用“截断反向传播算法”来应对RNN训练过程中的梯度计算问题。这种方法将序列分割为多个片段，在每个片段上进行前向计算和反向传播，而不是在整个序列上进行。此外，为了应对梯度爆炸的问题，可以使用梯度裁剪技术；为了应对梯度消失的问题，可以使用具有长期记忆能力的LSTM（长短期记忆）单元。

总而言之，RNN是一种适用于处理序列数据的神经网络模型，广泛应用于自然语言处理、语音识别等领域。随着研究的深入，人们对于RNN结构和性能的理解也在不断加深，同时也在探索更好的网络结构和训练算法。
